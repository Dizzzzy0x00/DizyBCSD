#  ***Binbert语义特征嵌入模型***

BinBERT 是基于预训练模型 CodeBERT 微调得到的嵌入模型，专门针对反汇编代码的语义特征进行学习，以实现跨架构二进制相似性检测等任务。

- 基础预训练模型——CodeBERT：CodeBERT 是微软发布的一款双语预训练模型，其架构基于 Transformer，并在大规模代码数据集上进行预训练。CodeBERT 的设计使其能捕捉代码的语法、语义和结构信息，因此非常适合作为代码领域的下游任务的基础模型。CodeBERT的模型结构如图所示，虽然 CodeBERT 在预训练阶段主要使用的是自然语言和主流编程语言（如 Python、Java 等）数据，训练数据中没有直接包含汇编语言，但CodeBERT 基于 Transformer 架构，其预训练过程中已经捕捉了程序语言中普遍存在的语法结构和模式。这种通用的代码表示能力有助于捕捉代码中普遍存在的模式和结构，尽管汇编语言在表达方式和语法上与高级语言有所不同，但它同样遵循一定的结构和规则。也就是说即便目标语言（汇编语言）在预训练数据中缺失，其学习到的代码特征仍能为下游任务提供一个较好的起点。通过微调，模型可以在已有知识的基础上逐步适应汇编语言的特点，这正是***\*迁移学习\****的优势所在。微调时如果使用足够且高质量的汇编语言数据集，模型可以重新调整其参数，使得嵌入向量更适合表达汇编语言的语义和结构。这种领域适应（Domain Adaptation）策略在很多跨领域应用中都证明是有效的。

![img](file:///C:\Users\jxi20\AppData\Local\Temp\ksohtml14464\wps2.jpg)

​	

- 数据集构建——三元组跨架构样本：为了构建跨架构二进制样本的三元组数据集，用于相似性检测任务时，为了降低不同编译器优化和编译器版本对结果的干扰，算法首先通过解析文件名，将相同函数（由编译器、源文件及函数名唯一确定）的样本进行分组，从而确保仅关注于跨架构场景下的相似性。算法具体实现如表Algorithm 3.1所示，具体来说，将样本按照（编译器，优化器，函数）三元组进行分组，算法对每一组样本生成正样本对，即从不同架构中选择两份反汇编数据作为锚点和正样本。在负样本的选择上，算法采用随机抽样策略，持续从总样本池中随机抽取，直到抽取到的负样本与当前正样本对的函数名不匹配为止，从而确保负样本与正样本在语义上具有区别。最终，每个三元组要求满足以下条件：

  - 锚点和正样本：必须来自同一函数的不同架构样本，保证跨架构相似性；

  -  锚点和负样本：必须来自不同函数，尽量排除跨优化和跨编译对相似性检测的影响。

    这种设计策略既突出了跨架构下的相似性检测，又通过减少其他因素的干扰，提高了数据集的质量，为后续对比学习和模型微调提供了更为精准和可靠的训练样本。

- 微调策略——基于 Triplet Loss 的对比学习：为了使 BinBERT 学习到更具区分性的语义特征，本研究采用了 Triplet Loss 进行对比学习。具体实现细节如下：通过计算锚点、正样本和负样本三者之间的欧式距离，模型被引导使得锚点与正样本的距离更近，而与负样本的距离更远，从而强化嵌入向量在语义空间中的区分性。代码中定义的 TripletLoss 类实现了这一损失函数，其中 margin 参数用于控制正负样本之间的最小距离差异。

## 参考文献

 PEWNY J, GARMANY B, GAWLIK R, 等. Cross-Architecture Bug Search in Binary Executables[C/OL]//2015 IEEE Symposium on Security and Privacy. 2015: 709-724[2025-01-06]. https://ieeexplore.ieee.org/document/7163056. DOI:[10.1109/SP.2015.49](https://doi.org/10.1109/SP.2015.49).